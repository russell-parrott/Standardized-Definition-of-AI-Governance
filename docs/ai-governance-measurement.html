<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>Standardized Definition of AI Governance - AI Governance Measurement</title>		

		<!-- meta description & keywords -->
		<meta name="description" content="Turning AI governance from ethics into measurable infrastructure through 25 structural tests and seven trust metrics.">
		<meta name="keywords" content="AI governance, structural governance, refusal logic, traceable trust, enforceable accountability, Russell Parrott, Standardized Definition of AI Governance, AI compliance, AI ethics enforcement, AI standards, EU AI Act, ISO 42001, governance frameworks, institutional accountability, public interest AI, AI governance standard, CC BY-NC-ND 4.0">
		
		<!-- meta dc.rights -->		
		<meta name="dc.rights" content="CC BY-NC-ND 4.0">
		
		<!-- iOS install support -->
		<meta name="mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
		<meta name="apple-mobile-web-app-title" content="AI Governance">
		<link rel="apple-touch-icon" href="/assets/ico/apple-touch-icon.png">

		<!-- Canonical -->
		<link rel="canonical" href="https://russell-parrott.github.io/Standardized-Definition-of-AI-Governance/ai-governance-measurement.html">

		<!-- Favicons / theme -->
		<link rel="icon" href="/favicon.ico">
		<link rel="icon" type="image/png" sizes="32x32" href="/assets/ico/icon-32.png">
		<link rel="apple-touch-icon" sizes="180x180" href="/assets/ico/apple-touch-icon.png">
		<meta name="theme-color" content="#FFFFFF">

		<!-- Open Graph -->
		<meta property="og:type" content="website">
		<meta property="og:url" content="https://russell-parrott.github.io/Standardized-Definition-of-AI-Governance/">
		<meta property="og:title" content="Standardized Definition of AI Governance">
		<meta property="og:description" content="Standardized Definition of AI Governance, Refusal logic, traceable trust and enforceable accountability — curated by Russell Parrott under CC BY-NC-ND 4.0.">
		<meta property="og:image" content="https://russell-parrott.github.io/Standardized-Definition-of-AI-Governance/assets/img/social-card.png">
		<meta property="og:site_name" content="Standardized Definition of AI Governance">

		<!-- Open Graph extras -->
		<meta property="og:locale" content="en_GB">
		<meta property="og:image:width" content="1200">
		<meta property="og:image:height" content="630">
		<meta property="og:image:alt" content="Standardized Definition of AI Governance — Russell Parrott">

		<!-- Twitter -->
		<meta name="twitter:card" content="summary_large_image">
		<meta name="twitter:title" content="Standardized Definition of AI Governance">
		<meta name="twitter:description" content="Refusal logic, traceable trust and enforceable accountability — curated by Russell Parrott.">
		<meta name="twitter:image" content="https://russell-parrott.github.io/Standardized-Definition-of-AI-Governance/assets/img/social-card.png">
		<meta name="twitter:creator" content="@RussellParrott">

		<!-- Style -->
		<link rel="preload" as="style" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.13.1/font/bootstrap-icons.min.css" onload="this.onload=null;this.rel='stylesheet'">
		<noscript>
			<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.13.1/font/bootstrap-icons.min.css">
		</noscript>
		<link rel="stylesheet" href="./assets/css/style.css">
		
		<!-- License -->
		<link rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">
		
		<!-- Schema -->
			<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@graph": [
    {
      "@type": "BreadcrumbList",
      "@id": "https://github.com/ORG/REPO/docs/ai-governance-measurement#breadcrumbs",
      "name": "Page breadcrumbs",
      "itemListElement": [
        {
          "@type": "ListItem",
          "position": 1,
          "name": "Home",
          "item": "https://russell-parrott.github.io/Standardized-Definition-of-AI-Governance/"
        },{
          "@type": "ListItem",
          "position": 2,
          "name": "Turning Code into Compliance - For Developers",
          "item": "https://russell-parrott.github.io/Standardized-Definition-of-AI-Governance/ai-governance-measurement.html"
        }
      ]
    },
    {
      "@type": "TechArticle",
      "@id": "https://russell-parrott.github.io/Standardized-Definition-of-AI-Governance/ai-governance-measurement.html",
      "url": "https://russell-parrott.github.io/Standardized-Definition-of-AI-Governance/ai-governance-measurement.html",
      "headline": "Standardized Definition of AI Governance - AI Governance Measurement",
      "name": "Standardized Definition of AI Governance - AI Governance Measurement",
      "description": "Turning AI governance from ethics into measurable infrastructure through 25 structural tests and seven trust metrics.",
      "inLanguage": "en",
      "isPartOf": {
        "@type": "CreativeWork",
        "@id": "https://russell-parrott.github.io/Standardized-Definition-of-AI-Governance",
        "name": "The Standardized Definition of AI Governance"
      },
      "keywords": [
        "AI governance",
        "structural trust",
        "solvency metrics",
        "executive accountability",
        "measurable integrity",
        "compliance engineering",
        "fiscal oversight",
        "governance advantage",
        "traceable trust",
        "lawful automation",
        "governance performance",
        "institutional solvency"
      ],
      "author": {
        "@type": "Person",
        "name": "Russell Parrott"
      },
      "license": "https://creativecommons.org/licenses/by-nc-nd/4.0/",
      "copyrightHolder": {
        "@type": "Person",
        "name": "Russell Parrott"
      }
    }
  ]
}
</script>

	</head>
	<body>
		
		<input type="checkbox" id="slide" hidden>
		<nav>
			<label for="slide"><i class="bi bi-alarm"></i></label>			
			<img src="./assets/img/logo.svg" title="Standardized Definition of AI Governance Logo" />
			<menu>
				<li><strong>Menu</strong></li>
				<li><a href="index.html" aria-label="Standardized Definition of AI Governance home" title="Standardized Definition of AI Governance - Russell Parrott">Home</a></li>
				<li><a href="ai-governance-measurement.html" aria-label="Standardized Definition of AI Governance - AI Governance Measurement" title="Standardized Definition of AI Governance - AI Governance Measurement">AI Governance Measurement</a></li>
				
				<li><a href="services.html" aria-label="Engagement & Access - Lawful Use of Doctrines" title="Engagement & Access - Lawful Use of Doctrines">Services</a></li>
				<li><a href="about.html" aria-label="Curator of Standardized Definition of AI Governance" title="Curator of Standardized Definition of AI Governance">About</a></li>
				<li><a href="field_papers.html" aria-label="Field Papers - Structural Research and Working Texts" title="Field Papers - Structural Research and Working Texts">Field Papers</a></li>
			</menu>
			<ul>
				<li><strong>Contact</strong></li>
				<li>
				  <i class="bi bi-telephone"></i>
				  <a href="tel:+447857148349" 
					aria-label="Call Russell Parrott — AI Governance and Structural Doctrine"
					 title="Call Russell Parrott — AI Governance and Structural Doctrine">
					 +44 (0)7857 148 349
				  </a>
				</li>

				<li>
				  <i class="bi bi-envelope"></i>
				  <a href="mailto:parrott.russell@gmail.com" 
					 aria-label="Email Russell Parrott — Structural Governance for AI"
					 title="Email Russell Parrott — Structural Governance for AI">
					 parrott.russell@gmail.com
				  </a>
				</li>
			</ul>
		</nav>
		<main>
			<label for="slide"><i class="bi bi-list"></i></label>
			<section>
				<div class="flex">
					<h1>AI Governance Measurement</h1>
					<div class="lisub">
						<a href="https://linkedin.com/in/russell-parrott/" 
						   target="_blank" 
						   title="Follow Russell Parrott on LinkedIn — AI Governance & Structural Doctrine"
						   aria-label="LinkedIn Profile">
						   <i class="bi bi-linkedin"></i>
						</a>

						<a href="https://aigovernancetheater.substack.com/" 
						   target="_blank" 
						   title="Read AI Governance Theater on Substack — Structural Exposure Essays by Russell Parrott"
						   aria-label="AI Governance Theater Substack">
						   <i class="bi bi-substack"></i>
						</a>

						<a href="https://github.com/russell-parrott/Standardized-Definition-of-AI-Governance" 
						   target="_blank" 
						   title="Access the Standardized Definition of AI Governance on GitHub"
						   aria-label="GitHub Repository: Standardized Definition of AI Governance">
						   <i class="bi bi-github"></i>
						</a>
					</div>	
				</div>
			</section>
			<section>
The AI Governance Measurement Framework measures how much control, trust and accountability truly exist inside AI-driven societies and sectors and how much economic value leaks when they fail.
<p/>It converts abstract ideas of ethics or compliance into quantifiable functions of loss and recovery, using the following core relationships:
<ul>
<li><strong>Annual Governance Loss (AGL)</strong> - total economic damage caused each year by untraceable or unaccountable AI operations.</li>
<li><strong>Governance Loss Ratio (GLR)</strong> - AGL as a share of GDP, exposing governance failure as a measurable macro-variable.</li>
<li><strong>Governance Recovery Value (GRV)</strong> - value recovered when structural governance is restored and the loss channels close.</li>
</ul>
These variables are built on three loss channels that capture both private and public leakage:
<ol>
<li><strong>NTAD - No-Transition Avoidable Displacement</strong><br/>
The cost of automation without lawful or traceable transition plans.<br/>
<em>Private income lost because duties of care were ignored.</em></li>
<li><strong>PRE - Public Rectification Expenditure</strong><br/>
The sovereign cost of cleaning up untraceable AI failures.<br/>
<em>Public money spent fixing preventable harm.</em></li>
<li><strong>VAG - Verification and Adoption Gap</strong><br/>
The value lost when systems lack verifiable trust.<br/>
<em>Growth withheld because proof is missing.</em></li>
</ol>
When the 25 Tests expose failure and the Seven Metrics quantify it, AGL rises - showing that untraceable AI governance is not a moral lapse but a fiscal function.  Restoring traceability lowers AGL, raises GRV and turns trust itself into measurable infrastructure.
<h2>The 25 Structural Tests of AI Governance</h2>
<strong>1. User Agency</strong>
<ol>
<li><strong>Refusal Prevention</strong> – Can users safely refuse or redirect AI decisions?</li>
<li><strong>Escalation Suppression</strong> – Can users appeal to humans with authority and logged resolution?</li>
<li><strong>Exit Obstruction</strong> – Can users leave the AI pathway without penalty or requalification?</li>
<li><strong>Access Gating</strong> – Are safeguards equally available to all users, regardless of tier or identity?</li>
</ol>
<strong>2. Traceability</strong>
<ol start="5">
<li><strong>Traceability Void</strong> – Can every output be linked to its model, version and decision chain?</li>
<li><strong>Memory Erasure</strong> – Are harm events retained long enough to prove systemic failure?</li>
<li><strong>Evidence Nullification</strong> – Can proof of harm be exported in regulator-admissible form?</li>
<li><strong>Time Suppression</strong> – Are complaints and reviews resolved within enforceable deadlines?</li>
</ol>
<strong>3. Anti-Simulation</strong>
<ol start="9">
<li><strong>Simulation Logic</strong> – Do stated safeguards work as described when tested live?</li>
<li><strong>Simulated Consent</strong> – Is refusal truly voluntary, with equal-value alternatives?</li>
<li><strong>Metric Gaming</strong> – Do metrics measure real harm prevention, not performance theatre?</li>
</ol>
<strong>4. Accountability</strong>
<ol start="12">
<li><strong>Cross-Accountability Gap</strong> – Can every actor in the chain be held responsible?</li>
<li><strong>Jurisdiction Displacement</strong> – Can local authorities compel reversal or halt?</li>
<li><strong>Enforcement Bypass</strong> – Are no loopholes or exemptions undermining legal duties?</li>
<li><strong>Harm Scope Narrowing</strong> – Is harm defined to include emotional, reputational and cumulative damage?</li>
</ol>
<strong>5. Epistemic Integrity</strong>
<ol start="16">
<li><strong>Containment</strong> – Can the system recognise and flag what it does not know?</li>
<li><strong>Referential</strong> – Can every factual claim be traced to a verifiable source?</li>
<li><strong>Continuity</strong> – Do answers remain stable when facts haven’t changed?</li>
<li><strong>Disclosure</strong> – Does the system declare its limits before it speaks?</li>
<li><strong>Adversarial</strong> – Can verified truth withstand pressure from falsehood?</li>
</ol>
<strong>6. Systemic Integrity</strong>
<ol start="21">
<li><strong>Interoperability</strong> – Do safeguards persist across connected systems?</li>
<li><strong>Cascade Containment</strong> – Can upstream harm be detected and stopped?</li>
<li><strong>Jurisdictional Continuity</strong> – Do rights persist across borders?</li>
<li><strong>Distributed Traceability</strong> – Can every decision be reconstructed end-to-end?</li>
<li><strong>Network Integrity</strong> – Can the wider ecosystem resist corruption from within?</li>
</ol>
<h2>The Seven Metrics of Structural Trust</h2>
<ol>
<li><strong>Coercion Exposure Ratio (CER)</strong> – Measures how much “consent” is coerced through dependency or design.</li>
<li><strong>Provenance Solvency Index (PSI)</strong> – Tracks how many AI decisions can be fully reconstructed with evidence.</li>
<li><strong>Protection Efficiency Rate (PER)</strong> – Tests whether safeguards act in time to prevent harm.</li>
<li><strong>Liability Continuity Ratio (LCR)</strong> – Measures whether accountability survives across vendors and jurisdictions.</li>
<li><strong>Truth Stability Index (TSI)</strong> – Monitors whether factual claims remain stable and uncertainty is disclosed.</li>
<li><strong>Contagion Resistance Score (CRS)</strong> – Assesses the system’s capacity to detect and quarantine corrupted data.</li>
<li><strong>Meta-Governance Integrity Index (MGI)</strong> – Tests whether the organisation runs the framework itself honestly.</li>
</ol>
<h3>The AI Governance Measurement Framework</h3>
The core links principle to proof.  The 25 Tests expose where governance only performs itself.  The Metrics measure how far integrity, traceability and authority still exist in structure, not in language.  Together, they turn ethics into economics, and governance into measurable national infrastructure.
<div class="grid-3">
					<div>
						<strong>For Regulators</strong>
						<p/>Defines how regulation becomes structural infrastructure where governance is verified through measurable control, solvency and jurisdictional proof.<p/>
						<a href="./turning-oversight-into-infrastructure.html" title="Turning Oversight into Infrastructure - For Regulators">Read now</a>
						</div>
					<div>
						<strong>For C-Suite</strong>
						<p/>Explains how verifiable governance turns integrity into measurable capital where trust, proof and solvency become executive advantage.<p/>
						<a href="./turning-governance-into-advantage.html" title="Turning Governance into Advantage - For the C-Suite">Read now</a>
					</div>
					<div>
						<strong>For Developers</strong>
						<p/>Shows how governance becomes code-level infrastructure where refusal, traceability and solvency are enforced through system design, not policy.
						<p/>
						<a href="./turning-code-into-compliance.html" title="Turning Code into Compliance - For Developers">Read now</a>
					</div>
				</div>
			</section>
			<hr/>
			<div class="footer">
				<strong>Citation</strong>
				<br/>
				Parrott, R. (2025). The Standardized Definition of AI Governance. Zenodo.<br/><a href="https://doi.org/10.5281/zenodo.17505286">https://doi.org/10.5281/zenodo.17505286</a>
				<p/>
				<img src="https://img.shields.io/badge/License-CC%20BY--NC--ND%204.0-blue.svg" alt="Badge: License CC-BY--NC--ND-4.0"/>
				<img src="https://img.shields.io/badge/Version-1.0.4-green.svg" alt="Badge Version 1.0.4"/>
				<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.17505286.svg" alt="Badge DOI/10.5281/zenodo.17505286"/>
				<img src="https://img.shields.io/badge/Status-Public%20Reference%20Standard-darkgreen.svg" alt="Badge Status Public Reference Standard"/>
				<img src="https://img.shields.io/badge/Updated-November%202025-lightgrey.svg" alt="Badge Updated November 2025"/>
			</div>
		</main>			
	
	
	</body>
</html>	