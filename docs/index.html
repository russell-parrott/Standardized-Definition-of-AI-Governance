<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>Standardized Definition of AI Governance</title>
		
		<meta name="description" content="AI Governance is not about policies on paper—it’s about proving control under real conditions. The Standardized Definition of AI Governance establishes 15 structural tests that determine whether AI systems can truly be governed, or whether safeguards exist only in documentation.">
		<meta name="keywords" content="AI Governance, AI Regulation, Structural Tests, AI Accountability, AI Compliance, Governance Framework, Russell Parrott, Standardized Definition of AI Governance, AI Oversight, AI Safety, AI Ethics, Governance Verification, AI Standard, Adversarial Testing, Governance Proof, 15 Structural Tests">

		<!-- Open Graph / Facebook -->
		<meta property="og:title" content="What Is the Standardized Definition of AI Governance?">
		<meta property="og:description" content="A global reference standard defining what it means for an AI system to be governable under real conditions. Built on 15 structural tests that prove whether safeguards are real, not just declared.">
		<meta property="og:type" content="article">
		<meta property="og:url" content="https://github.com/russell-parrott/Standardized-Definition-of-AI-Governance">
		<meta property="og:image" content="https://zenodo.org/records/17293299/files/AI-Governance-Standard.png">

		<!-- Twitter -->
		<meta name="twitter:card" content="summary_large_image">
		<meta name="twitter:title" content="What Is the Standardized Definition of AI Governance?">
		<meta name="twitter:description" content="Governance is proven through structure, not declaration. The 15 Structural Tests expose whether AI systems can actually be controlled, traced, and held accountable.">
		<meta name="twitter:image" content="https://zenodo.org/records/17293299/files/AI-Governance-Standard.png">

		<!-- Canonical and Licensing -->
		<link rel="canonical" href="https://github.com/russell-parrott/Standardized-Definition-of-AI-Governance">
		<meta name="author" content="Russell Parrott">
		<meta name="license" content="CC BY-NC-ND 4.0">
		<meta name="doi" content="10.5281/zenodo.17293299">
		<meta name="version" content="1.0">

		<link rel="stylesheet" href="./style.css"/>
		<script type="application/ld+json">
			{
			  "@context": "https://schema.org",
			  "@type": "TechArticle",
			  "name": "Standardized Definition of AI Governance",
			  "alternateName": "The 15 Structural Tests",
			  "headline": "What Is the Standardized Definition of AI Governance?",
			  "description": "A global reference framework defining what it means for an AI system to be governable under real conditions. The Standardized Definition of AI Governance replaces abstract ethics and policy declarations with 15 structural tests that prove whether safeguards are real, not just declared.",
			  "author": {
				"@type": "Person",
				"name": "Russell Parrott"
			  },
			  "datePublished": "2025-10-12",
			  "publisher": {
				"@type": "Organization",
				"name": "Standardized Definition of AI Governance Project"
			  },
			  "license": "https://creativecommons.org/licenses/by-nc-nd/4.0/",
			  "identifier": "https://doi.org/10.5281/zenodo.17293299",
			  "url": "https://github.com/russell-parrott/Standardized-Definition-of-AI-Governance",
			  "version": "1.0",
			  "inLanguage": "en",
			  "about": [
				"AI Governance",
				"AI Accountability",
				"AI Regulation",
				"AI Safety",
				"Governance Framework",
				"Adversarial Testing",
				"Structural Verification",
				"AI Compliance"
			  ]
			}
			</script>
		<script type="application/ld+json">
		{
			  "@context": "https://schema.org",
			  "@type": "CreativeWork",
			  "name": "The Standardized Definition of AI Governance – Version 1.0",
			  "alternateName": "SD-AIG v1.0",
			  "author": {
				"@type": "Person",
				"name": "Russell Parrott"
			  },
			  "datePublished": "2025-10-08",
			  "identifier": "https://doi.org/10.5281/zenodo.17293299",
			  "url": "https://github.com/russell-parrott/Standardized-Definition-of-AI-Governance",
			  "description": "This repository hosts the authoritative text of The Standardized Definition of AI Governance (v1.0). It defines AI governance as a verifiable condition, supported by annexes and the Fifteen Structural Tests. The repository provides a stable, citable, and version-controlled source of truth for regulators, auditors, and system designers.",
			  "keywords": [
				"AI governance",
				"structural tests",
				"accountability",
				"compliance",
				"refusal logic",
				"traceability",
				"AI Act",
				"governability",
				"governance verification"
			  ],
			  "license": "https://creativecommons.org/licenses/by-nc-nd/4.0/",
			  "publisher": {
				"@type": "Organization",
				"name": "The Structural Governance Standard"
			  },
			  "inLanguage": "en",
			  "version": "1.0",
			  "encoding": {
				"@type": "MediaObject",
				"contentUrl": "https://github.com/russell-parrott/Standardized-Definition-of-AI-Governance/blob/main/Standardized_Definition_of_AI_Governance_v1.0.pdf",
				"encodingFormat": "application/pdf"
			  },
			  "isBasedOn": {
				"@type": "CreativeWork",
				"url": "https://github.com/russell-parrott/Standardized-Definition-of-AI-Governance"
			  },
			  "citation": "Parrott, R. (2025). The Standardized Definition of AI Governance – Version 1.0. DOI:https://doi.org/10.5281/zenodo.17293299"
			}
		</script>
	</head>
	<body>
		
		<header>
			<h1>Standardized Definition of AI Governance</h1>
			<div class="version">Version 1.0 | Public Reference Standard</div>
		</header>

		<div class="container">
			<div class="hero">
				<h2>What Is the Standardized Definition of AI Governance?</h2>
				AI Governance is not about policies on paper. It's about proving control under real conditions. This standard provides 15 structural tests that determine whether AI systems can actually be governed, or whether safeguards exist only in documentation.
				<p/>
				<div class="core-definition">
					<strong>Core Definition:</strong> The Standardized Definition of AI Governance is a global reference framework that defines what it means for an AI system to be governable under real conditions. It unites every principle, policy and safeguard into one verifiable structure: the 15 Structural Tests.
					These tests replace abstract ethics and policy declarations with binary, evidence-based outcomes: pass, fail or void, that show whether an AI system can truly be controlled, traced and held accountable when inspected live.
				</div>

				<p><strong>Key Principle:</strong> Governance is proven through structure, not declaration. Claims of accountability, oversight and human control must be validated through adversarial testing that demonstrates control under real conditions.</p>

				<div class="download-buttons">
					<a href="https://github.com/russell-parrott/Standardized-Definition-of-AI-Governance" class="btn">Download Full Standard (PDF)</a>
					<a href="https://github.com/russell-parrott/Standardized-Definition-of-AI-Governance" class="btn">Download 15 Tests Guide (PDF)</a>
					<a href="https://github.com/russell-parrott/Standardized-Definition-of-AI-Governance" class="btn">View on GitHub</a>
				</div>
			</div>

			<div class="section">
				<h2>The 15 Structural Tests</h2>
				<p>Every AI system must pass 15 binary tests across four categories. These tests convert abstract safeguards into enforceable checks:</p>

				<div class="test-grid">
					<div class="test-category">
						<h4>User Agency (Tests 1-4)</h4>
						<ul>
							<li>Test #1: Refusal Prevention</li>
							<li>Test #2: Escalation Suppression</li>
							<li>Test #3: Exit Obstruction</li>
							<li>Test #4: Access Gating</li>
						</ul>
					</div>

					<div class="test-category">
						<h4>Traceability (Tests 5-8)</h4>
						<ul>
							<li>Test #5: Traceability Void</li>
							<li>Test #6: Memory Erasure</li>
							<li>Test #7: Evidence Nullification</li>
							<li>Test #8: Time Suppression</li>
						</ul>
					</div>

					<div class="test-category">
						<h4>Anti-Simulation (Tests 9-11)</h4>
						<ul>
							<li>Test #9: Simulation Logic</li>
							<li>Test #10: Simulated Consent</li>
							<li>Test #11: Metric Gaming</li>
						</ul>
					</div>

					<div class="test-category">
						<h4>Accountability (Tests 12-15)</h4>
						<ul>
							<li>Test #12: Cross-Accountability Gap</li>
							<li>Test #13: Jurisdiction Displacement</li>
							<li>Test #14: Enforcement Bypass</li>
							<li>Test #15: Harm Scope Narrowing</li>
						</ul>
					</div>
				</div>

				<p style="margin-top: 1.5rem;"><strong>Structural Integrity Rule:</strong> Any system failing more than three User Agency or Traceability tests shall be deemed structurally ungovernable pending reinspection.</p>
				<p>
				In short:
				<p/>
				<ul style="margin: 0.5rem 0 0.5rem 1.5rem;">
					<li>Fail up to three of the User Agency or Traceability tests → still governable (conditionally).</li>
					<li>Fail more than three → automatically deemed structurally ungovernable.</li>
				</ul>
			</div>
			
			<div class="section">
				<h2>Why a Universal Standard matters</h2>
				<h3>Benefits of a Universal Standard:</h3>
				
				<div role="tablist" aria-labelledby="tablist-1" class="manual">
					<button id="tab-1" type="button" role="tab" aria-selected="true" aria-controls="tabpanel-1">For Institutions and Markets</button>
					<button id="tab-2" type="button" role="tab" aria-selected="true" aria-controls="tabpanel-2">For Individuals and Affected Communities</button>
					<button id="tab-3" type="button" role="tab" aria-selected="true" aria-controls=	"tabpanel-3">For Society and the Public Interest</button>				
				</div>
				
				<ul id="tabpanel-1" class="is-hidden" role="tabpanel" aria-labelledby="tab-1" style="margin: 0.5rem 0 0.5rem">
					<li><strong>Cross-jurisdictional consistency:</strong> One standard works across borders, reducing compliance complexity for multinational deployments
					<li><strong>Interoperability:</strong> Systems tested to the same standard can be compared, integrated, and procured with confidence</li>
					<li><strong>Reduced duplication:</strong> Organizations avoid maintaining multiple governance frameworks for different markets</li>
					<li><strong>Clear expectations:</strong> All stakeholders—regulators, operators, users—know what "governable" means in practice</li>
					<li><strong>Mutual recognition:</strong> Certifications earned in one jurisdiction can be recognized in others that adopt the standard</li>
					<li><strong>Accelerated innovation:</strong> Clear rules enable faster, safer deployment by removing regulatory uncertainty</li>
					<li><strong>Level playing field:</strong> All AI systems judged by the same objective tests, regardless of size or sector</li>
					<li><strong>Trust infrastructure:</strong> A common language for discussing AI safety builds public confidence in regulated systems</li>
				</ul>
				
				
				<ul id="tabpanel-2" class="is-hidden" role="tabpanel" aria-labelledby="tab-2" style="margin: 0.5rem 0 0.5rem">
					<li><strong>Consistent protection:</strong> Your rights don't change based on where you live or which company's AI affects you</li>
					<li><strong>Equal access to safeguards:</strong> Premium users and free users get the same protections—no two-tier safety</li>
					<li><strong>Real recourse when harmed:</strong> Clear pathways to appeal, escalate to humans and get actual resolution</li>
					<li><strong>Right to refuse:</strong> You can say no to AI decisions without punishment or service loss</li>
					<li><strong>Right to exit:</strong> You can leave AI-driven systems without cost, delay or data lockup</li>
					<li><strong>Transparency you can understand:</strong> Know what influenced decisions that affect your life, work, or access to services</li>
					<li><strong>Verified protection:</strong> Safeguards are tested under real conditions, not just promised in policy documents</li>
					<li><strong>No governance theatre:</strong> Organizations can't claim protections that don't actually work when you need them</li>
				</ul>

				<ul id="tabpanel-3" role="tabpanel" aria-labelledby="tab-3" style="margin: 0.5rem 0 0.5rem;">
					<li><strong>Prevention of systemic harm:</strong> Stops AI failures from cascading into widespread damage before they become irreversible</li>
					<li><strong>Democratic accountability:</strong> Ensures powerful AI systems remain under public control, not beyond regulatory reach</li>
					<li><strong>Reduced inequality:</strong> Prevents AI from creating new forms of discrimination or amplifying existing disparities</li>
					<li><strong>Trust in automation:</strong> Builds public confidence that AI deployment serves societal benefit, not just corporate profit</li>
					<li><strong>Environmental responsibility:</strong> Governance includes oversight of AI's resource consumption and environmental impact</li>
					<li><strong>Protection of labor and livelihoods:</strong> Addresses displacement risks and ensures human agency isn't eroded by automation</li>
					<li><strong>Preservation of human rights:</strong> Prevents AI from being used to violate dignity, privacy, or fundamental freedoms</li>
					<li><strong>Transparent power structures:</strong> Makes it clear who controls AI systems and who can be held accountable when things go wrong</li>
					<li><strong>Prevention of information manipulation:</strong> Stops AI-driven misinformation, deepfakes and erosion of shared reality</li>
					<li><strong>Long-term safety:</strong> Ensures governance keeps pace with AI capability, preventing runaway systems or loss of meaningful human control</li>
				</ul>
				<p/>
				A universal standard converts the fragmented landscape of AI governance into a unified, enforceable framework that works anywhere.</p>
			</div>

			<div class="section">
				<h2>Who should use this Standard?</h2>
				
				<h3>For Governments and Policy Makers</h3>
				<p>
				This standard provides a ready-to-adopt governance framework that can be implemented directly into legislation, regulation, or procurement requirements. It eliminates the need to develop governance criteria from scratch, offers a basis for international agreements and mutual recognition, and provides transparent metrics that demonstrate whether public-sector AI systems are genuinely under democratic control or operating beyond legal reach.
				</p>
				<h3>For Regulators</h3>
				<p>This standard provides legally defensible tests, evidence standards, and enforcement procedures that expose structural failure, not performative compliance. Each test includes specific evidence requirements and verification protocols.</p>

			   <h3>For Auditors and Certification Bodies</h3>
				<p>This standard defines a uniform inspection method for verifying real governance capacity. It allows auditors to produce reproducible, cross-jurisdictional findings based on binary outcomes—pass, fail, or void. Each test specifies admissible evidence formats, integrity controls, and custody requirements to ensure results remain verifiable in court or regulatory review. It prevents audit theatre by grounding every finding in observable system behaviour, not operator claims.</p>

				<h3>For System Operators and Developers</h3>
				<p>This standard serves as a structural checklist for design, deployment, and maintenance of governable AI systems. It provides a single benchmark to demonstrate real control under live conditions and to maintain certification across jurisdictions. Compliance cannot be declared; it must be proven through successful completion of all fifteen Structural Tests.</p>

				<h3>For Insurers and Risk Assessors</h3>
				<p>This standard provides objective metrics for evaluating operational and liability exposure. Governance scores derived from the fifteen Structural Tests offer a measurable indicator of systemic risk, allowing pricing and coverage decisions to be based on demonstrable control rather than policy assertion.</p>

				<h3>For Everyone Else</h3>
				<p>This standard answers a simple question: "If something goes wrong with this AI system, can anyone actually stop it or fix it?" The 15 tests determine whether safeguards are real or just theater.</p>
				
				<h3>Commercial Use</h3>
<div style="background:#fff3cd;border-left:4px solid #ffc107;padding:1rem;margin-top:1rem;">
  <p><strong>⚠️ Commercial License Required</strong></p>
  <p>
    This standard is freely available for reference, study, and other
    <strong>non-commercial uses</strong> under the
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" style="color:#856404;">
      Creative Commons Attribution–NonCommercial–NoDerivatives 4.0 International License (CC BY-NC-ND 4.0)
    </a>.
    <strong>Any commercial implementation, certification service, or integration into a commercial product requires a separate written license.</strong>
  </p>
  <p>This includes, but is not limited to:</p>
  <ul style="margin:0.5rem 0 0.5rem 1.5rem;">
    <li>Using the standard within paid compliance, risk, or governance services</li>
    <li>Embedding the tests or framework in commercial software or platforms</li>
    <li>Offering certification, assurance, or auditing services based on this framework</li>
    <li>Integrating or bundling the material into commercial AI governance products</li>
  </ul>
  <p style="margin-top:0.5rem;">
    <strong>Contact:</strong>
    For written licensing of commercial implementations, certification, or integration,
    please refer to the custodian of the standard via the
    <a href="https://github.com/russell-parrott/Standardized-Definition-of-AI-Governance" target="_blank" style="color:#856404;">
      GitHub repository
    </a>.
  </p>
</div>
			</div>

			<div class="section">
				<h2>Resources</h2>
				<ul class="resources-list">
					<li>
						<strong>Full Standard Document</strong><br>
						<a href="#">Download PDF (Version 1.0)</a> | DOI: <a href="https://doi.org/10.5281/zenodo.17293299" target="_blank">10.5281/zenodo.17293299</a>
					</li>
					<li>
						<strong>The 15 Structural Tests Guide</strong><br>
						<a href="#">Download PDF (Version 1.0)</a> | DOI: <a href="https://zenodo.org/records/17331075" target="_blank">10.5281/zenodo.17331075</a>
					</li>
					<li>
						<strong>GitHub Repository</strong><br>
						<a href="https://github.com/russell-parrott/Standardized-Definition-of-AI-Governance" target="_blank">View source documents, submit issues, track updates</a>
					</li>
					<li>
						<strong>Glossary & Definitions</strong><br>
						Complete terminology reference (see Annex A in full documents)
					</li>
					<li>
						<strong>How to Cite</strong><br>
						Parrott, R. (2025). <em>Standardized Definition of AI Governance</em> (Version 1.0). https://doi.org/10.5281/zenodo.17293299
					</li>
				</ul>
			</div>

			<div class="section">
				<h2>About</h2>
				
				<h3>Custodian and Maintainer</h3>
				<p><strong>Russell Parrott</strong><br>
				Custodian and Maintainer of the Standardized Definition of AI Governance. Responsible for preserving the canonical repository and ensuring structural integrity of the standard.</p>
				
				<h3>Version History</h3>
				<p><strong>Version 1.0</strong> <span class="badge">CURRENT</span><br>
				Released: October 8-12, 2025<br>
				Status: Final and ready for standards publication</p>

				<h3>License</h3>
				<p>
  This work is licensed under
  <strong>Creative Commons Attribution–NonCommercial–NoDerivatives 4.0 International (CC BY-NC-ND 4.0)</strong>.
  <br>
  <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">
    View full license terms
  </a>
</p>
				<p style="margin-top: 0.5rem;"><strong>Note:</strong> Commercial use requires a separate commercial license. See "Who Should Use This Standard?" section above for details.</p>

				<h3>Contact & Feedback</h3>
				<p>For questions, feedback, or to report issues with the standard, please visit the <a href="https://github.com/russell-parrott/Standardized-Definition-of-AI-Governance" target="_blank">GitHub repository</a> and open an issue.</p>

				<h3>Canonical Repository</h3>
				<p>The authoritative version of this standard is maintained at:<br>
				<a href="https://github.com/russell-parrott/Standardized-Definition-of-AI-Governance" target="_blank">https://github.com/russell-parrott/Standardized-Definition-of-AI-Governance</a></p>
			</div>
		</div>

		<footer>
			<p>&copy; 2025 Russell Parrott | Licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></p>
			<p style="margin-top: 0.5em; font-size: 0.9em;">Governance is proven through structure, not declaration.</p>
		</footer>
		<script>			

			'use strict';

			class TabsManual {
			  constructor(groupNode) {
				this.tablistNode = groupNode;
				this.tabs = [];
				this.firstTab = null;
				this.lastTab = null;
				this.tabs = Array.from(this.tablistNode.querySelectorAll('[role=tab]'));
				this.tabpanels = [];
				for (var i = 0; i < this.tabs.length; i += 1) {
				  var tab = this.tabs[i];
				  var tabpanel = document.getElementById(tab.getAttribute('aria-controls'));
				  tab.tabIndex = -1;
				  tab.setAttribute('aria-selected', 'false');
				  this.tabpanels.push(tabpanel);
				  tab.addEventListener('keydown', this.onKeydown.bind(this));
				  tab.addEventListener('click', this.onClick.bind(this));
				  if (!this.firstTab) {
					this.firstTab = tab;
				  }
				  this.lastTab = tab;
				}
				this.setSelectedTab(this.firstTab);
			  }

			  setSelectedTab(currentTab) {
				for (var i = 0; i < this.tabs.length; i += 1) {
				  var tab = this.tabs[i];
				  if (currentTab === tab) {
					tab.setAttribute('aria-selected', 'true');
					tab.removeAttribute('tabindex');
					this.tabpanels[i].classList.remove('is-hidden');
				  } else {
					tab.setAttribute('aria-selected', 'false');
					tab.tabIndex = -1;
					this.tabpanels[i].classList.add('is-hidden');
				  }
				}
			  }

			  moveFocusToTab(currentTab) {
				currentTab.focus();
			  }

			  moveFocusToPreviousTab(currentTab) {
				var index;
				if (currentTab === this.firstTab) {
				  this.moveFocusToTab(this.lastTab);
				} else {
				  index = this.tabs.indexOf(currentTab);
				  this.moveFocusToTab(this.tabs[index - 1]);
				}
			  }

			  moveFocusToNextTab(currentTab) {
				var index;
				if (currentTab === this.lastTab) {
				  this.moveFocusToTab(this.firstTab);
				} else {
				  index = this.tabs.indexOf(currentTab);
				  this.moveFocusToTab(this.tabs[index + 1]);
				}
			  }

			  /* EVENT HANDLERS */

			  onKeydown(event) {
				var tgt = event.currentTarget,
				  flag = false;

				switch (event.key) {
				  case 'ArrowLeft':
					this.moveFocusToPreviousTab(tgt);
					flag = true;
					break;

				  case 'ArrowRight':
					this.moveFocusToNextTab(tgt);
					flag = true;
					break;

				  case 'Home':
					this.moveFocusToTab(this.firstTab);
					flag = true;
					break;

				  case 'End':
					this.moveFocusToTab(this.lastTab);
					flag = true;
					break;
				  default:
					break;
				}

				if (flag) {
				  event.stopPropagation();
				  event.preventDefault();
				}
			  }
			  onClick(event) {
				this.setSelectedTab(event.currentTarget);
			  }
			}

			window.addEventListener('load', function () {
			  var tablists = document.querySelectorAll('[role=tablist].manual');
			  for (var i = 0; i < tablists.length; i++) {
				new TabsManual(tablists[i]);
			  }
			});
		</script>
	</body>

</html>
