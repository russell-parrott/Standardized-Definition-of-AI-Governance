<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>AI Accountability When Evidence Is Required | Russell Parrott</title>
		<meta name="robots" content="index, follow, max-snippet:-1, max-image-preview:large">
		
		<meta name="description" content="AI accountability analysis focused on evidence. Independent standards and explainers showing why governance claims fail once outcomes are challenged and proof is required." />
		<meta name="keywords" content="AI accountability, AI governance failure, AI evidence, evidentiary accountability, auditability, traceability, AI decision reconstruction, public accountability standards, AI logs and retention, post-incident analysis, disputed AI outcomes, accountability without access, governance capability vs intent" />
		
		<!-- meta dc.rights -->		
		<meta name="dc.rights" content="CC BY-NC-ND 4.0">		

		<!-- Open Graph -->
		<meta property="og:type" content="website">
		<meta property="og:url" content="https://russellparrott.eu">
		<meta property="og:title" content="AI Accountability When Evidence Is Required | Russell Parrott">
		<meta property="og:description" content="Independent analysis of why AI governance assurances collapse once evidence is demanded after dispute or harm.">
		<meta property="og:site_name" content="AI accountability fails when evidence is required">

		<!-- Open Graph extras -->
		<meta property="og:locale" content="en_GB">	
		
		<!-- Canonical -->
		<link rel="canonical" href="https://russellparrott.eu">
		
		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Raleway:ital,wght@0,100..900;1,100..900&family=Roboto:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
		
		<link href="https://unpkg.com/aos@2.3.1/dist/aos.css" rel="stylesheet">
		
		<link href="style.css" rel="stylesheet">
		<script type="application/ld+json">
		{
  "@context": "https://schema.org",
  "@graph": [
    {
      "@type": "Person",
      "@id": "https://russellparrott.eu/#person",
      "name": "Russell Parrott",
      "jobTitle":"Analyst, Researcher and Author",
      "description": "Independent analyst, researcher and author focused on why AI governance and accountability claims fail when outcomes are challenged and evidence is required. Works from publicly observable material without relying on access, cooperation or trust. Does not provide advice, certification, legal interpretation, or compliance opinions. Publishes public accountability standards defining minimum conditions for accountability once an outcome is disputed; publishes 1,500–2,000 word explainers (not standards or case studies) via Amazon KDP; and short LinkedIn posts highlighting specific failure points and assumptions.",
      "url": "https://russellparrott.eu/",
      "homeLocation": {
        "@type": "Place",
        "name": "Agia Efimia, Greece"
      },
	   "knowsAbout": [
        "AI accountability",
        "Evidence and reconstruction",
        "Auditability and traceability",
        "Governance capability vs intent",
        "Public-only accountability standards",
        "Dispute-driven accountability",
        "Decision provenance",
        "System logging and retention limits",
        "Post-incident evidentiary constraints"
      ],
	  "hasOccupation": [
        {
          "@type": "Occupation",
          "name": "Analyst",
          "description": "Analyses why AI accountability claims fail when evidence is demanded after contested outcomes."
        },
        {
          "@type": "Occupation",
          "name": "Researcher",
          "description": "Develops public accountability standards and examines structural limits of governance under dispute conditions."
        },
        {
          "@type": "Occupation",
          "name": "Author",
          "description": "Publishes explainers and public standards on AI accountability and evidentiary failure modes."
        }
      ],
	  "publishingPrinciples": "https://russellparrott.eu/#principles",
      "makesOffer": [
        {
          "@type": "Offer",
          "itemOffered": {
            "@type": "CreativeWork",
            "name": "Public accountability standards (AI accountability)",
            "description": "Public standards defining minimum conditions that must already exist for accountability to be possible once an AI-related outcome is challenged. Define boundaries; do not assess compliance, recommend practices, or propose solutions."
          }
        },
        {
          "@type": "Offer",
          "itemOffered": {
            "@type": "Article",
            "name": "Explainers (1,500–2,000 words)",
            "description": "Long-form explainers that unpack specific accountability failures, assumptions or structural limits in plain terms. Not standards or case studies. Published as standalone briefs on Amazon (KDP)."
          }
        },
        {
		  "@type": "Offer",
		  "itemOffered": {
			"@type": "Service",
			"name": "Short posts (LinkedIn)",
			"description": "Short posts surfacing a single idea, boundary or failure point, often pointing to longer explainers or underlying standards.",
			"provider": { "@id": "https://russellparrott.eu/#person" },
			"areaServed": "Worldwide"
		  }
		}
      ],
      "sameAs": [
        "https://www.linkedin.com/in/russell-parrott/",
        "https://amazon.com/author/russellparrott"
      ]
    },
    {
      "@type": "WebSite",
      "@id": "https://russellparrott.eu/#website",
      "url": "https://russellparrott.eu/",
      "name": "Russell Parrott",
      "description": "Independent analysis of AI accountability failures when outcomes are challenged and evidence is required.",
      "inLanguage": "en",
      "publisher": {
        "@id": "https://russellparrott.eu/#person"
      },
	  "keywords": [
        "AI accountability",
        "AI governance failure",
        "structural evidence",
        "algorithmic accountability",
        "evidence production in AI systems",
		"traceability limits",
		"event reconstruction",
        "public evidence assessment",
		"accountability failure patterns",
		"governance claims analysis"
      ]	  
    },
    {
      "@type": "WebPage",
      "@id": "https://russellparrott.eu/#homepage",
      "url": "https://russellparrott.eu/",
      "name": "Russell Parrott — AI Accountability",
      "isPartOf": {
        "@id": "https://russellparrott.eu/#website"
      },
      "about": {
        "@id": "https://russellparrott.eu/#person"
      },
      "mainEntity": {
        "@id": "https://russellparrott.eu/#books"
      },
      "inLanguage": "en"
    },
    {
      "@type": "ItemList",
      "@id": "https://russellparrott.eu/#books",
      "name": "Books",
      "itemListOrder": "https://schema.org/ItemListOrderAscending",
      "numberOfItems": 4,
      "itemListElement": [
        {
          "@type": "ListItem",
          "position": 1,
          "item": {
            "@type": "Book",
            "@id": "https://www.amazon.com/dp/B0GCT2WR9W#book",
            "name": "Structural Governance Failure: A Recognition Guide",
            "url": "https://www.amazon.com/dp/B0GCT2WR9W",
            "author": {
              "@id": "https://russellparrott.eu/#person"
            },
            "description": "Identifies recurring structural conditions that prevent accountability, focusing on recognising failures without internal access or proposed remedies.",
           "bookFormat": "https://schema.org/Paperback",
            "inLanguage": "en",
            "genre": ["AI governance", "accountability", "systems analysis"],
            "isAccessibleForFree": false
          }
        },
        {
          "@type": "ListItem",
          "position": 2,
          "item": {
            "@type": "Book",
            "@id": "https://www.amazon.com/dp/B0FNJQQQ9K#book",
            "name": "The Structural Governance Standard for AI: Exposing what AI Systems really do",
            "url": "https://www.amazon.com/dp/B0FNJQQQ9K",
            "author": {
              "@id": "https://russellparrott.eu/#person"
            },
            "description": "Operational framework to expose whether AI systems uphold real safeguards using 15 enforceable checks.",
           "bookFormat": "https://schema.org/Paperback",
            "inLanguage": "en",
            "genre": ["AI governance", "assurance", "accountability"],
            "isAccessibleForFree": false
          }
        },
        {
          "@type": "ListItem",
          "position": 3,
          "item": {
            "@type": "Book",
            "@id": "https://www.amazon.com/dp/B0F1ZVYBYL#book",
            "name": "The AI World Order: Intelligence, Power, and the New Global Conflict",
            "url": "https://www.amazon.com/dp/B0F1ZVYBYL",
            "author": {
              "@id": "https://russellparrott.eu/#person"
            },
            "description": "Examines AI as a weapon and economic force, arguing that control of algorithms will determine the next global superpower.",
           "bookFormat": "https://schema.org/Paperback",
            "inLanguage": "en",
            "genre": ["geopolitics", "technology policy", "AI"],
            "isAccessibleForFree": false
          }
        },
        {
          "@type": "ListItem",
          "position": 4,
          "item": {
            "@type": "Book",
            "@id": "https://www.amazon.com/dp/B0G5LK173F#book",
            "name": "Below 40: The Hidden Fragility of AI Systems",
            "url": "https://www.amazon.com/dp/B0G5LK173F",
            "author": {
              "@id": "https://russellparrott.eu/#person"
            },
            "description": "Reveals how most AI systems cannot prove how they work and introduces the Evidential Resilience Ratio to measure traceability and resilience.",
           "bookFormat": "https://schema.org/Paperback",
            "inLanguage": "en",
            "genre": ["AI accountability", "traceability", "systems resilience"],
            "isAccessibleForFree": false
          }
        }
      ]
    }
  ]
}
		</script>
	</head>
	<body>
		
		<header>
			<a href="mailto:parrott.russell@gmail.com" title="Email Russell Parrott" ><svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" class="bi bi-envelope" viewBox="0 0 16 16"><path d="M0 4a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2zm2-1a1 1 0 0 0-1 1v.217l7 4.2 7-4.2V4a1 1 0 0 0-1-1zm13 2.383-4.708 2.825L15 11.105zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 0 0 2 13h12a1 1 0 0 0 .966-.741M1 11.105l4.708-2.897L1 5.383z"/>
			</svg> parrott.russell@gmail.com</a>
			<a href="tel:+447857148389" title="Phone Russell Parrott"><svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" class="bi bi-telephone" viewBox="0 0 16 16"><path d="M3.654 1.328a.678.678 0 0 0-1.015-.063L1.605 2.3c-.483.484-.661 1.169-.45 1.77a17.6 17.6 0 0 0 4.168 6.608 17.6 17.6 0 0 0 6.608 4.168c.601.211 1.286.033 1.77-.45l1.034-1.034a.678.678 0 0 0-.063-1.015l-2.307-1.794a.68.68 0 0 0-.58-.122l-2.19.547a1.75 1.75 0 0 1-1.657-.459L5.482 8.062a1.75 1.75 0 0 1-.46-1.657l.548-2.19a.68.68 0 0 0-.122-.58zM1.884.511a1.745 1.745 0 0 1 2.612.163L6.29 2.98c.329.423.445.974.315 1.494l-.547 2.19a.68.68 0 0 0 .178.643l2.457 2.457a.68.68 0 0 0 .644.178l2.189-.547a1.75 1.75 0 0 1 1.494.315l2.306 1.794c.829.645.905 1.87.163 2.611l-1.034 1.034c-.74.74-1.846 1.065-2.877.702a18.6 18.6 0 0 1-7.01-4.42 18.6 18.6 0 0 1-4.42-7.009c-.362-1.03-.037-2.137.703-2.877z"/></svg>+44(0) 785 7148 389</a>
			<a href="tel:+447857148389" title="Contact Russell Parrott via Signal"><svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" class="bi bi-signal" viewBox="0 0 16 16"><path d="m6.08.234.179.727a7.3 7.3 0 0 0-2.01.832l-.383-.643A7.9 7.9 0 0 1 6.079.234zm3.84 0L9.742.96a7.3 7.3 0 0 1 2.01.832l.388-.643A8 8 0 0 0 9.92.234m-8.77 3.63a8 8 0 0 0-.916 2.215l.727.18a7.3 7.3 0 0 1 .832-2.01l-.643-.386zM.75 8a7 7 0 0 1 .081-1.086L.091 6.8a8 8 0 0 0 0 2.398l.74-.112A7 7 0 0 1 .75 8m11.384 6.848-.384-.64a7.2 7.2 0 0 1-2.007.831l.18.728a8 8 0 0 0 2.211-.919M15.251 8q0 .547-.082 1.086l.74.112a8 8 0 0 0 0-2.398l-.74.114q.082.54.082 1.086m.516 1.918-.728-.18a7.3 7.3 0 0 1-.832 2.012l.643.387a8 8 0 0 0 .917-2.219m-6.68 5.25c-.72.11-1.453.11-2.173 0l-.112.742a8 8 0 0 0 2.396 0l-.112-.741zm4.75-2.868a7.2 7.2 0 0 1-1.537 1.534l.446.605a8 8 0 0 0 1.695-1.689zM12.3 2.163c.587.432 1.105.95 1.537 1.537l.604-.45a8 8 0 0 0-1.69-1.691zM2.163 3.7A7.2 7.2 0 0 1 3.7 2.163l-.45-.604a8 8 0 0 0-1.691 1.69l.604.45zm12.688.163-.644.387c.377.623.658 1.3.832 2.007l.728-.18a8 8 0 0 0-.916-2.214M6.913.831a7.3 7.3 0 0 1 2.172 0l.112-.74a8 8 0 0 0-2.396 0zM2.547 14.64 1 15l.36-1.549-.729-.17-.361 1.548a.75.75 0 0 0 .9.902l1.548-.357zM.786 12.612l.732.168.25-1.073A7.2 7.2 0 0 1 .96 9.74l-.727.18a8 8 0 0 0 .736 1.902l-.184.79zm3.5 1.623-1.073.25.17.731.79-.184c.6.327 1.239.574 1.902.737l.18-.728a7.2 7.2 0 0 1-1.962-.811zM8 1.5a6.5 6.5 0 0 0-6.498 6.502 6.5 6.5 0 0 0 .998 3.455l-.625 2.668L4.54 13.5a6.502 6.502 0 0 0 6.93-11A6.5 6.5 0 0 0 8 1.5"/></svg>+44(0) 785 7148 389</a>
		</header>					
		
		<main>
			<section>	
				
				<div class="default hero">		
					<img src="1744104855832.png" alt="Russell Parrott" class="hero-img"/>
					<div class="hero-text">
						<small>Russell Parrott</small>
						Analyst<br/>Researcher<br/>Author
					</div>	
				</div>			
					
				<div class="default" data-aos="fade-up-right" data-aos-delay="150">
					<h1>AI accountability fails when evidence is required.</h1>  
					<p/>
					I analyse why AI governance and accountability claims fail when outcomes are challenged and evidence is required.
					<p/>					
					Most organisations claim to have AI governance. They publish policies, assign responsibility, conduct audits and issue compliance statements. Yet when AI-driven systems cause harm or produce disputed outcomes those assurances often fail to show what actually happened.
					<p/>
					The problem is rarely bad faith. It is structural.
				</div>
				
				<div class="default" data-aos="fade-up-left">
					Most governance material describes intent rather than capability. Policies state what should occur, but they do not establish what a system can technically record, retain, reconstruct or demonstrate once an outcome is questioned.
					<p/>
					When an AI-related decision is challenged, accountability depends on evidence: whether the system can show how an outcome was produced, what constraints applied at the time and where responsibility was technically anchored.
					<p/>
					Without that evidence, governance claims cannot be relied upon regardless of how complete the documentation appears.
				</div>
				
				<div class="default" data-aos="fade-up-right">					
					I do not advise organisations, certify systems, interpret law, or provide compliance opinions. I publish independent analysis that explains where AI accountability fails in practice and why those failures persist despite audits, frameworks and regulatory claims.
				</div>
			</section>				

			<section>				
				<h2>What I do</h2>
				<div id="what-i-do">
					Most AI governance work assumes that accountability can be exercised when needed.  My work asks a prior question: can accountability still exist once evidence is actually demanded?
					<p/>
					I approach AI accountability as an evidentiary problem; not a policy or ethics one.
					<p/>  
					My core work is the development of public accountability standards.  These set out the minimum conditions that must already be in place for AI accountability to be possible once an outcome is challenged.  I work solely from publicly observable material without relying on access, cooperation or trust; a self-imposed constraint that exists because accountability is only real if it can be exercised by someone who does not control the system.
					<p/>
					Courts, regulators, journalists, insurers and affected individuals all have to rely on what is already observable in public because disputes arise precisely when trust has broken down and cooperation can no longer be assumed. In those moments, accountability does not depend on what a system owner is willing to disclose after the fact, only on evidence that exists independently of access, permission or goodwill.
					<p/>
					The standards do not assess compliance, recommend governance practices or propose solutions.  They define boundaries: the point at which responsibility, evidence or procedure can no longer operate, regardless of intent or policy.
					<p/>
					Alongside these standards, I publish long-form explainers of around 1,500–2,000 words.  Each unpacks specific accountability failures, assumptions or structural limits in plain terms. They are neither standards or case studies, rather they explain why and where accountability breaks when scrutiny begins. 
					<p/>
					In parallel, I write short posts on Linkedin that surface a single idea, boundary or failure point. These posts exist to signal, illustrate or provoke attention to an accountability issue, often pointing back to the longer explainers or to the underlying standards.
					<p/>
					In short:
					<ul style="margin:1rem;" >
					<li>My Standards define what must exist for accountability to be possible at all.</li>
					<li>Explainers clarify how and why accountability fails in practice.</li>
					<li>Short posts highlight specific points of failure or assumption.</li>
					</ul>
					All three address the same problem from different distances, none replace the others.
					<p/>
					My work is not about how AI governance should look.  It is about what can still be proven once something has gone wrong, and what cannot.
				</div>
			</section>			
		
			<section>
				<h3>AI Accountability</h3>
				<div class="carousel-holder">					
					<div class="carousel">
						<img src="414-10KN62L._SY425_.jpg" alt="Public AI Accountability: A Reference Standard for Third-Party Reliance"/>
						<div>
						<strong>Public AI Accountability: A Reference Standard for Third-Party Reliance</strong>
						<p/>
						Defines the minimum publicly visible conditions that must already exist for AI accountability to be possible in practice.
						<p/>
						<a href="https://www.amazon.com/dp/B0GH7QNL46" title="Public AI Accountability: A Reference Standard for Third-Party Reliance" target="blank">https://www.amazon.com/dp/B0GH7QNL46</a>
						</div>
					</div>
				
					<div class="carousel">
						<img src="51T6IsDskvL._SL1430_.jpg" alt="Structural Governance Failure: A Recognition Guide"/>
						<div>
						<strong>Structural Governance Failure: A Recognition Guide</strong>
						<p/>
						Identifies recurring structural conditions that prevent accountability, focusing on recognising failures without internal access or proposed remedies.
						<p/>
						<a href="https://www.amazon.com/dp/B0GCT2WR9W" title="Structural Governance Failure: A Recognition Guide" target="blank">https://www.amazon.com/dp/B0GCT2WR9W</a>
						</div>
					</div>
					<div class="carousel">
						<img src="612-utgC9UL._SL1430_.jpg" alt="The Structural Governance Standard for AI: Exposing what AI Systems really do"/>
						<div><strong>The Structural Governance Standard for AI: Exposing what AI Systems really do</strong>
						<p/>
						The first operational framework to expose whether AI systems uphold real safeguards using 15 enforceable checks.
						<p/>
						<a href="https://www.amazon.com/dp/B0FNJQQQ9K" title="The Structural Governance Standard for AI: Exposing what AI Systems really do" target="blank">https://www.amazon.com/dp/B0FNJQQQ9K</a>
						</div>
					</div>
					<div class="carousel">
						<img src="71trNzMdNSL._SL1430_.jpg"  alt="The AI World Order: Intelligence, Power, and the New Global Conflict"/>
						<div><strong>The AI World Order: Intelligence, Power, and the New Global Conflict</strong>
						<p/>
						Examines AI as a weapon and economic force, arguing that control of algorithms will determine the next global superpower.
						<p/>
						<a href="https://www.amazon.com/dp/B0F1ZVYBYL" title="The AI World Order: Intelligence, Power, and the New Global Conflict" target="blank">https://www.amazon.com/dp/B0F1ZVYBYL</a>
						</div>
					</div>
					<div class="carousel">
						<img src="61m6FEidA8L._SL1430_.jpg" alt="Below 40: The Hidden Fragility of AI Systems"/>
						<div><strong>Below 40: The Hidden Fragility of AI Systems</strong>
						<p/>
						Reveals how most AI systems cannot prove how they work and introduces the Evidential Resilience Ratio to measure traceability and resilience.
						<p/>
						<a href="https://www.amazon.com/dp/B0G5LK173F" title="Below 40: The Hidden Fragility of AI Systems" target="blank">https://www.amazon.com/dp/B0G5LK173F</a>
						</div>
					</div>
				</div>	
			</section>
			<section>	
				<h4>Get in touch</h4>
				<form method="post" action="javascript.void(0)" id="form-enquiry">
					
					<div class="fields">
						You’re welcome to contact me about my writing, AI Accountability Evidence Reports or for press and media enquiries.									
						<label for="name">Name
						<input type="text" name="name" id="name" placeholder="Your name">
						</label>	
						<label for="email">Email
						<input type="text" name="email" id="email" placeholder="Your email address">
						</label>	
						<label for="enquiry">Reason for contact</label>
						<select name="enquiry" id="enquiry">
							<option value="">Select</option>
							<option>AI Accountability Evidence Report enquiry</option>
							<option>Pre-litigation or evidentiary viability enquiry</option>
							<option>Press or media request</option>
							<option>Academic or research correspondence</option>
							<option>Rights, permissions, or citation request</option>
							<option>Published work clarification (non-advisory)</option>
							<option>Other</option>
						</select>
						<label for="message">Message</label>
						<textarea name="message" id="message" rows="5"></textarea>						
						<button type="submit" title="Send a message">Send Message</button>
						<small style="display:block;font-size:80%;margin-top:1rem">By submitting this form you consent to the processing of your information for the purpose of responding to your enquiry.</small>
					</div>
					<div class="success-message" hidden >
						<h5>Thank You!</h5>
						I have received your email and will get back to you as soon as possible.
						<p/>
						I appreciate you taking the time to reach out
					</div>
					<ul class="contact">						
						<li>
							<strong>Address</strong>
							<br/>							
							Agia Efimia<br/>Kefalonia<br/>28081
							<br/>Greece
							</span>
						</li>
						<li>
							<strong>Email</strong>
							<br/>
							<a href="mailto:parrott.russell@gmail.com" title="Email Russell Parrott">parrott.russell@gmail.com</a>
						</li>
						<li>
							<strong>Phone</strong>
							<br/>
							<span><a href="tel:+447857148389" title="Phone Russell Parrott">+44(0) 785 7148 389</a></span>
						</li>
						<li>
							<strong>Calendar</strong>
							<br/>
							<span><a href="https://calendar.app.google/j5vk4me3HibGTHHR7" title="Book an appointment - my Calendar">https://calendar.app.google/j5vk4me3HibGTHHR7</a></span>						
						<li>
							<strong>Linkedin</strong>
							<br/>
							<span><a href="linkedin.com/in/russell-parrott" title="Russell Parrott on Linkedin" target="blank">https://linkedin.com/in/russell-parrott</a></span>
						</li>							
					</ul>
				</form>
			</section>
		</main>	
		
		<footer>
			&copy; 2026 Russell Parrott - All Rights Reserved
			<p/>
			Privacy notice
			<p/>
			This website collects only the information you choose to submit via the contact form (name, email address and message). The data is used solely to respond to your enquiry and is not shared or used for marketing. Messages are retained only as long as necessary to handle the correspondence. You may request access to or deletion of your data at any time by emailing parrott.russell@gmail.com
			<p/>
			The data controller is: Russell Parrott, Agia Efimia, 28081, Kefalonia, Greece.
			<p/>
			This site does not use cookies or tracking technologies.
			<p/>
			Disclaimer
			<p/>
			The material on this site is independent analytical commentary. It does not constitute legal, regulatory, compliance, financial or professional advice. No representations or warranties are made as to completeness or suitability for any purpose. No professional relationship is created through use of this site and no liability is accepted for decisions made in reliance on its content.
		</footer>

		<script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script>
		<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@emailjs/browser@4/dist/email.min.js"></script>
		
		<script>
			"use strict"
						
			AOS.init();			

			emailjs.init({	publicKey: "3Bb98Nyu6Gi7MoJqx" });
			document.getElementById('form-enquiry').addEventListener('submit', function (evt) {
				evt.preventDefault(); 
				var templateParams = {
					name: document.getElementById('name').value,
					enquiry: document.getElementById('enquiry').value,
					message: document.getElementById('message').value,
					email: document.getElementById('email').value
				};
				emailjs.send("service_7adde56", "template_zn882e5", templateParams)
				.then((response) => {
					console.log('SUCCESS!', response.status, response.text);
					document.querySelector('.fields').setAttribute('hidden', '');
					document.querySelector('.success-message').removeAttribute('hidden');
					document.getElementById('form-enquiry').reset()
				}).catch((error) => {
					console.log('FAILED...', error);
				});
			});
			
		</script>
	</body>
</html>